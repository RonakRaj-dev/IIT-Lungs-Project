{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-10T11:24:24.268760Z",
     "start_time": "2026-02-10T11:23:48.677430Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T11:28:08.944616Z",
     "start_time": "2026-02-10T11:28:08.941183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PREPROCESSED_DIR = r\"C:\\Users\\gamer\\OneDrive\\Desktop\\IIT Project\\Datasets\\datasets\\Pilot\\preprocessed_image\"\n",
    "CSV_PATH = r\"C:\\Users\\gamer\\OneDrive\\Desktop\\IIT Project\\Datasets\\datasets\\Pilot\\train_final.csv\"\n",
    "IMG_SIZE = 512"
   ],
   "id": "973f602659999f60",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T11:24:30.848249Z",
     "start_time": "2026-02-10T11:24:30.842309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_noise_reduction(image):\n",
    "    denoised = cv2.medianBlur(image, 3)\n",
    "    denoised = cv2.GaussianBlur(denoised, (3, 3), 0)\n",
    "    return denoised\n",
    "\n",
    "def apply_normalization(image):\n",
    "    img = image.astype('float32') / 255.0\n",
    "    mean, std = np.mean(img), np.std(img)\n",
    "    return (img - mean) / (std + 1e-7)\n",
    "\n",
    "# 2. Build Extractor (Your Code)\n",
    "def build_feature_extractor():\n",
    "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model.output\n",
    "    feature_vector = GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=base_model.input, outputs=feature_vector)"
   ],
   "id": "80b4c696002f3044",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T14:07:09.223738Z",
     "start_time": "2026-02-10T11:31:39.940613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openvino.tools.ovc.moc_frontend import extractor\n",
    "feature_extractor = build_feature_extractor()\n",
    "# 1. Load and CLEAN the Dataframe\n",
    "train_df = pd.read_csv(CSV_PATH)\n",
    "# CRITICAL FIX: Remove any leading/trailing spaces from the filenames\n",
    "train_df['Image Index'] = train_df['Image Index'].str.strip()\n",
    "\n",
    "# 2. Path Debugging (Checks if the folder is actually visible to Python)\n",
    "if not os.path.exists(PREPROCESSED_DIR):\n",
    "    print(f\"âŒ ERROR: The directory '{PREPROCESSED_DIR}' does not exist.\")\n",
    "else:\n",
    "    sample_files = os.listdir(PREPROCESSED_DIR)[:5]\n",
    "    print(f\"âœ… Folder found! Sample files in folder: {sample_files}\")\n",
    "\n",
    "# 3. Extraction Loop\n",
    "train_features = []\n",
    "processed_indices = []\n",
    "\n",
    "print(f\"ğŸš€ Starting Extraction for {len(train_df)} images...\")\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    img_name = row['Image Index']\n",
    "    # Use abspath to ensure Python isn't getting confused by relative paths\n",
    "    img_path = os.path.abspath(os.path.join(PREPROCESSED_DIR, img_name))\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # A. Load Grayscale\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if img is None:\n",
    "            print(f\"âŒ Could not read image: {img_name} (File might be corrupted)\")\n",
    "            continue\n",
    "\n",
    "        # B. Apply Preprocessing\n",
    "        img = apply_noise_reduction(img)\n",
    "        img = apply_normalization(img)\n",
    "\n",
    "        # C. Prep for DenseNet (Resize & convert to 3-channel RGB)\n",
    "        img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img_rgb = cv2.merge([img_resized, img_resized, img_resized])\n",
    "        img_input = np.expand_dims(img_rgb, axis=0)\n",
    "\n",
    "        # D. Extract Features\n",
    "        feat = feature_extractor.predict(img_input, verbose=0)\n",
    "        train_features.append(feat.flatten())\n",
    "        processed_indices.append(index)\n",
    "\n",
    "        if len(processed_indices) % 100 == 0:\n",
    "            print(f\"âœ… Extracted features for {len(processed_indices)} images...\")\n",
    "    else:\n",
    "        # Debugging print to see exactly what Python is looking for\n",
    "        if index < 5: # Only print first 5 failures to avoid clutter\n",
    "            print(f\"âš ï¸ Not Found: Checked path -> {img_path}\")\n",
    "\n",
    "# Final Wrap up\n",
    "if train_features:\n",
    "    train_features_arr = np.array(train_features)\n",
    "    # Save the features so you don't have to extract them again!\n",
    "    np.save('extracted_features_test.npy', train_features_arr)\n",
    "    print(f\"âœ¨ Success! Saved feature matrix of shape {train_features_arr.shape}\")\n",
    "else:\n",
    "    print(\"âŒ No images were processed. Please check if the filenames in your CSV include '.png'.\")"
   ],
   "id": "e7e7cc0b93ef06b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Folder found! Sample files in folder: ['00000003_000.png', '00000003_001.png', '00000003_002.png', '00000003_003.png', '00000003_004.png']\n",
      "ğŸš€ Starting Extraction for 28080 images...\n",
      "âœ… Extracted features for 100 images...\n",
      "âœ… Extracted features for 200 images...\n",
      "âœ… Extracted features for 300 images...\n",
      "âœ… Extracted features for 400 images...\n",
      "âœ… Extracted features for 500 images...\n",
      "âœ… Extracted features for 600 images...\n",
      "âœ… Extracted features for 700 images...\n",
      "âœ… Extracted features for 800 images...\n",
      "âœ… Extracted features for 900 images...\n",
      "âœ… Extracted features for 1000 images...\n",
      "âœ… Extracted features for 1100 images...\n",
      "âœ… Extracted features for 1200 images...\n",
      "âœ… Extracted features for 1300 images...\n",
      "âœ… Extracted features for 1400 images...\n",
      "âœ… Extracted features for 1500 images...\n",
      "âœ… Extracted features for 1600 images...\n",
      "âœ… Extracted features for 1700 images...\n",
      "âœ… Extracted features for 1800 images...\n",
      "âœ… Extracted features for 1900 images...\n",
      "âœ… Extracted features for 2000 images...\n",
      "âœ… Extracted features for 2100 images...\n",
      "âœ… Extracted features for 2200 images...\n",
      "âœ… Extracted features for 2300 images...\n",
      "âœ… Extracted features for 2400 images...\n",
      "âœ… Extracted features for 2500 images...\n",
      "âœ… Extracted features for 2600 images...\n",
      "âœ… Extracted features for 2700 images...\n",
      "âœ… Extracted features for 2800 images...\n",
      "âœ… Extracted features for 2900 images...\n",
      "âœ… Extracted features for 3000 images...\n",
      "âœ… Extracted features for 3100 images...\n",
      "âœ… Extracted features for 3200 images...\n",
      "âœ… Extracted features for 3300 images...\n",
      "âœ… Extracted features for 3400 images...\n",
      "âœ… Extracted features for 3500 images...\n",
      "âœ… Extracted features for 3600 images...\n",
      "âœ… Extracted features for 3700 images...\n",
      "âœ… Extracted features for 3800 images...\n",
      "âœ… Extracted features for 3900 images...\n",
      "âœ… Extracted features for 4000 images...\n",
      "âœ… Extracted features for 4100 images...\n",
      "âœ… Extracted features for 4200 images...\n",
      "âœ… Extracted features for 4300 images...\n",
      "âœ… Extracted features for 4400 images...\n",
      "âœ… Extracted features for 4500 images...\n",
      "âœ… Extracted features for 4600 images...\n",
      "âœ… Extracted features for 4700 images...\n",
      "âœ… Extracted features for 4800 images...\n",
      "âœ… Extracted features for 4900 images...\n",
      "âœ… Extracted features for 5000 images...\n",
      "âœ… Extracted features for 5100 images...\n",
      "âœ… Extracted features for 5200 images...\n",
      "âœ… Extracted features for 5300 images...\n",
      "âœ… Extracted features for 5400 images...\n",
      "âœ… Extracted features for 5500 images...\n",
      "âœ… Extracted features for 5600 images...\n",
      "âœ… Extracted features for 5700 images...\n",
      "âœ… Extracted features for 5800 images...\n",
      "âœ… Extracted features for 5900 images...\n",
      "âœ… Extracted features for 6000 images...\n",
      "âœ… Extracted features for 6100 images...\n",
      "âœ… Extracted features for 6200 images...\n",
      "âœ… Extracted features for 6300 images...\n",
      "âœ… Extracted features for 6400 images...\n",
      "âœ… Extracted features for 6500 images...\n",
      "âœ… Extracted features for 6600 images...\n",
      "âœ… Extracted features for 6700 images...\n",
      "âœ… Extracted features for 6800 images...\n",
      "âœ… Extracted features for 6900 images...\n",
      "âœ… Extracted features for 7000 images...\n",
      "âœ… Extracted features for 7100 images...\n",
      "âœ… Extracted features for 7200 images...\n",
      "âœ… Extracted features for 7300 images...\n",
      "âœ… Extracted features for 7400 images...\n",
      "âœ… Extracted features for 7500 images...\n",
      "âœ… Extracted features for 7600 images...\n",
      "âœ… Extracted features for 7700 images...\n",
      "âœ… Extracted features for 7800 images...\n",
      "âœ… Extracted features for 7900 images...\n",
      "âœ… Extracted features for 8000 images...\n",
      "âœ… Extracted features for 8100 images...\n",
      "âœ… Extracted features for 8200 images...\n",
      "âœ… Extracted features for 8300 images...\n",
      "âœ… Extracted features for 8400 images...\n",
      "âœ… Extracted features for 8500 images...\n",
      "âœ… Extracted features for 8600 images...\n",
      "âœ… Extracted features for 8700 images...\n",
      "âœ… Extracted features for 8800 images...\n",
      "âœ… Extracted features for 8900 images...\n",
      "âœ… Extracted features for 9000 images...\n",
      "âœ… Extracted features for 9100 images...\n",
      "âœ… Extracted features for 9200 images...\n",
      "âœ… Extracted features for 9300 images...\n",
      "âœ… Extracted features for 9400 images...\n",
      "âœ… Extracted features for 9500 images...\n",
      "âœ… Extracted features for 9600 images...\n",
      "âœ… Extracted features for 9700 images...\n",
      "âœ… Extracted features for 9800 images...\n",
      "âœ… Extracted features for 9900 images...\n",
      "âœ… Extracted features for 10000 images...\n",
      "âœ… Extracted features for 10100 images...\n",
      "âœ… Extracted features for 10200 images...\n",
      "âœ… Extracted features for 10300 images...\n",
      "âœ… Extracted features for 10400 images...\n",
      "âœ… Extracted features for 10500 images...\n",
      "âœ… Extracted features for 10600 images...\n",
      "âœ… Extracted features for 10700 images...\n",
      "âœ… Extracted features for 10800 images...\n",
      "âœ… Extracted features for 10900 images...\n",
      "âœ… Extracted features for 11000 images...\n",
      "âœ… Extracted features for 11100 images...\n",
      "âœ… Extracted features for 11200 images...\n",
      "âœ… Extracted features for 11300 images...\n",
      "âœ… Extracted features for 11400 images...\n",
      "âœ… Extracted features for 11500 images...\n",
      "âœ… Extracted features for 11600 images...\n",
      "âœ… Extracted features for 11700 images...\n",
      "âœ… Extracted features for 11800 images...\n",
      "âœ… Extracted features for 11900 images...\n",
      "âœ… Extracted features for 12000 images...\n",
      "âœ… Extracted features for 12100 images...\n",
      "âœ… Extracted features for 12200 images...\n",
      "âœ… Extracted features for 12300 images...\n",
      "âœ… Extracted features for 12400 images...\n",
      "âœ… Extracted features for 12500 images...\n",
      "âœ… Extracted features for 12600 images...\n",
      "âœ… Extracted features for 12700 images...\n",
      "âœ… Extracted features for 12800 images...\n",
      "âœ… Extracted features for 12900 images...\n",
      "âœ… Extracted features for 13000 images...\n",
      "âœ… Extracted features for 13100 images...\n",
      "âœ… Extracted features for 13200 images...\n",
      "âœ… Extracted features for 13300 images...\n",
      "âœ… Extracted features for 13400 images...\n",
      "âœ… Extracted features for 13500 images...\n",
      "âœ… Extracted features for 13600 images...\n",
      "âœ… Extracted features for 13700 images...\n",
      "âœ… Extracted features for 13800 images...\n",
      "âœ… Extracted features for 13900 images...\n",
      "âœ… Extracted features for 14000 images...\n",
      "âœ… Extracted features for 14100 images...\n",
      "âœ… Extracted features for 14200 images...\n",
      "âœ… Extracted features for 14300 images...\n",
      "âœ… Extracted features for 14400 images...\n",
      "âœ… Extracted features for 14500 images...\n",
      "âœ… Extracted features for 14600 images...\n",
      "âœ… Extracted features for 14700 images...\n",
      "âœ… Extracted features for 14800 images...\n",
      "âœ… Extracted features for 14900 images...\n",
      "âœ… Extracted features for 15000 images...\n",
      "âœ… Extracted features for 15100 images...\n",
      "âœ… Extracted features for 15200 images...\n",
      "âœ… Extracted features for 15300 images...\n",
      "âœ… Extracted features for 15400 images...\n",
      "âœ… Extracted features for 15500 images...\n",
      "âœ… Extracted features for 15600 images...\n",
      "âœ… Extracted features for 15700 images...\n",
      "âœ… Extracted features for 15800 images...\n",
      "âœ… Extracted features for 15900 images...\n",
      "âœ… Extracted features for 16000 images...\n",
      "âœ… Extracted features for 16100 images...\n",
      "âœ… Extracted features for 16200 images...\n",
      "âœ… Extracted features for 16300 images...\n",
      "âœ… Extracted features for 16400 images...\n",
      "âœ… Extracted features for 16500 images...\n",
      "âœ… Extracted features for 16600 images...\n",
      "âœ… Extracted features for 16700 images...\n",
      "âœ… Extracted features for 16800 images...\n",
      "âœ… Extracted features for 16900 images...\n",
      "âœ… Extracted features for 17000 images...\n",
      "âœ… Extracted features for 17100 images...\n",
      "âœ… Extracted features for 17200 images...\n",
      "âœ… Extracted features for 17300 images...\n",
      "âœ… Extracted features for 17400 images...\n",
      "âœ… Extracted features for 17500 images...\n",
      "âœ… Extracted features for 17600 images...\n",
      "âœ… Extracted features for 17700 images...\n",
      "âœ… Extracted features for 17800 images...\n",
      "âœ… Extracted features for 17900 images...\n",
      "âœ… Extracted features for 18000 images...\n",
      "âœ… Extracted features for 18100 images...\n",
      "âœ… Extracted features for 18200 images...\n",
      "âœ… Extracted features for 18300 images...\n",
      "âœ… Extracted features for 18400 images...\n",
      "âœ… Extracted features for 18500 images...\n",
      "âœ… Extracted features for 18600 images...\n",
      "âœ… Extracted features for 18700 images...\n",
      "âœ… Extracted features for 18800 images...\n",
      "âœ… Extracted features for 18900 images...\n",
      "âœ… Extracted features for 19000 images...\n",
      "âœ… Extracted features for 19100 images...\n",
      "âœ… Extracted features for 19200 images...\n",
      "âœ… Extracted features for 19300 images...\n",
      "âœ… Extracted features for 19400 images...\n",
      "âœ… Extracted features for 19500 images...\n",
      "âœ… Extracted features for 19600 images...\n",
      "âœ… Extracted features for 19700 images...\n",
      "âœ… Extracted features for 19800 images...\n",
      "âœ… Extracted features for 19900 images...\n",
      "âœ… Extracted features for 20000 images...\n",
      "âœ… Extracted features for 20100 images...\n",
      "âœ… Extracted features for 20200 images...\n",
      "âœ… Extracted features for 20300 images...\n",
      "âœ… Extracted features for 20400 images...\n",
      "âœ… Extracted features for 20500 images...\n",
      "âœ… Extracted features for 20600 images...\n",
      "âœ… Extracted features for 20700 images...\n",
      "âœ… Extracted features for 20800 images...\n",
      "âœ… Extracted features for 20900 images...\n",
      "âœ… Extracted features for 21000 images...\n",
      "âœ… Extracted features for 21100 images...\n",
      "âœ… Extracted features for 21200 images...\n",
      "âœ… Extracted features for 21300 images...\n",
      "âœ… Extracted features for 21400 images...\n",
      "âœ… Extracted features for 21500 images...\n",
      "âœ… Extracted features for 21600 images...\n",
      "âœ… Extracted features for 21700 images...\n",
      "âœ… Extracted features for 21800 images...\n",
      "âœ… Extracted features for 21900 images...\n",
      "âœ… Extracted features for 22000 images...\n",
      "âœ… Extracted features for 22100 images...\n",
      "âœ… Extracted features for 22200 images...\n",
      "âœ… Extracted features for 22300 images...\n",
      "âœ… Extracted features for 22400 images...\n",
      "âœ… Extracted features for 22500 images...\n",
      "âœ… Extracted features for 22600 images...\n",
      "âœ… Extracted features for 22700 images...\n",
      "âœ… Extracted features for 22800 images...\n",
      "âœ… Extracted features for 22900 images...\n",
      "âœ… Extracted features for 23000 images...\n",
      "âœ… Extracted features for 23100 images...\n",
      "âœ… Extracted features for 23200 images...\n",
      "âœ… Extracted features for 23300 images...\n",
      "âœ… Extracted features for 23400 images...\n",
      "âœ… Extracted features for 23500 images...\n",
      "âœ… Extracted features for 23600 images...\n",
      "âœ… Extracted features for 23700 images...\n",
      "âœ… Extracted features for 23800 images...\n",
      "âœ… Extracted features for 23900 images...\n",
      "âœ… Extracted features for 24000 images...\n",
      "âœ… Extracted features for 24100 images...\n",
      "âœ… Extracted features for 24200 images...\n",
      "âœ… Extracted features for 24300 images...\n",
      "âœ… Extracted features for 24400 images...\n",
      "âœ… Extracted features for 24500 images...\n",
      "âœ… Extracted features for 24600 images...\n",
      "âœ… Extracted features for 24700 images...\n",
      "âœ… Extracted features for 24800 images...\n",
      "âœ… Extracted features for 24900 images...\n",
      "âœ… Extracted features for 25000 images...\n",
      "âœ… Extracted features for 25100 images...\n",
      "âœ… Extracted features for 25200 images...\n",
      "âœ… Extracted features for 25300 images...\n",
      "âœ… Extracted features for 25400 images...\n",
      "âœ… Extracted features for 25500 images...\n",
      "âœ… Extracted features for 25600 images...\n",
      "âœ… Extracted features for 25700 images...\n",
      "âœ… Extracted features for 25800 images...\n",
      "âœ… Extracted features for 25900 images...\n",
      "âœ… Extracted features for 26000 images...\n",
      "âœ… Extracted features for 26100 images...\n",
      "âœ… Extracted features for 26200 images...\n",
      "âœ… Extracted features for 26300 images...\n",
      "âœ… Extracted features for 26400 images...\n",
      "âœ… Extracted features for 26500 images...\n",
      "âœ… Extracted features for 26600 images...\n",
      "âœ… Extracted features for 26700 images...\n",
      "âœ… Extracted features for 26800 images...\n",
      "âœ… Extracted features for 26900 images...\n",
      "âœ… Extracted features for 27000 images...\n",
      "âœ… Extracted features for 27100 images...\n",
      "âœ… Extracted features for 27200 images...\n",
      "âœ… Extracted features for 27300 images...\n",
      "âœ… Extracted features for 27400 images...\n",
      "âœ… Extracted features for 27500 images...\n",
      "âœ… Extracted features for 27600 images...\n",
      "âœ… Extracted features for 27700 images...\n",
      "âœ… Extracted features for 27800 images...\n",
      "âœ… Extracted features for 27900 images...\n",
      "âœ… Extracted features for 28000 images...\n",
      "âœ¨ Success! Saved feature matrix of shape (28080, 1024)\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
